{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ICDAR 2023 Competition on Robust Layout Segmentation in Corporate Documents \u00b6 Info This competition concluded on April 3 rd , 2023 23:59 UTC+1:00. We would like to thank all participants for their contributions to the competition! Check the results on our EvalAI challenge page . Introduction \u00b6 In this competition, we challenge you to advance the research in accurately segmenting the layout in a very broad range of document styles and domains. Converting documents into a machine-processable format is an on-going challenge due to their huge variability in formats and complex structure. Recovering the layout structure and content from documents has remained a key problem since decades, and is as relevant as ever in 2023. To this date, a highly generalising model for structure and layout understanding has yet to be achieved. To raise the bar over previous competitions, we propose our newly published, human-annotated DocLayNet data-set as the base for a new challenge on various documents from corporate, technical and law domains. News \u00b6 Apr. 26 th , 2023 The ground-truth of our competition dataset is now available here . Apr. 3 rd , 2023 The competition has concluded. Check the results here . Mar. 27 th , 2023 The competition deadline is extended until April 3 rd , 2023 23:59 UTC+1:00 Mar. 20 th , 2023 The competition deadline is extended until March 26 th , 2023 12:00 AM (midnight) Jan. 13 th , 2023 The competition-dataset is now released and submissions are opened on EvalAI. Register here . Dec. 19 th , 2022 This competition is now live and will run until March 20 th , 2023. Find the detailed schedule here . Task and resources \u00b6 We invite you to develop a model that can accurately segment the layout components in document pages as bounding boxes on our competition data-set . The layout prediction accuracy you achieve with your solution will be evaluated on this dataset with our human-annotated layout ground-truth. Code submissions are not required. Find the details here . We highly recommend you to use our recently published DocLayNet dataset for training and internal validation. DocLayNet is highly diverse in layout coverage, and includes Financial reports, Patents, Manuals, Laws, Tenders and Technical Papers. It is human-annotated with 11 distinct layout class labels. Added value for model development is provided through the original PDF pages and a paired JSON representation of the text cells. Participation \u00b6 Everyone is welcome to participate in this competition. To ensure fairness, we require teams to abide by the participation rules .","title":"Home"},{"location":"#icdar-2023-competition-on-robust-layout-segmentation-in-corporate-documents","text":"Info This competition concluded on April 3 rd , 2023 23:59 UTC+1:00. We would like to thank all participants for their contributions to the competition! Check the results on our EvalAI challenge page .","title":"ICDAR 2023 Competition on Robust Layout Segmentation in Corporate Documents"},{"location":"#introduction","text":"In this competition, we challenge you to advance the research in accurately segmenting the layout in a very broad range of document styles and domains. Converting documents into a machine-processable format is an on-going challenge due to their huge variability in formats and complex structure. Recovering the layout structure and content from documents has remained a key problem since decades, and is as relevant as ever in 2023. To this date, a highly generalising model for structure and layout understanding has yet to be achieved. To raise the bar over previous competitions, we propose our newly published, human-annotated DocLayNet data-set as the base for a new challenge on various documents from corporate, technical and law domains.","title":"Introduction"},{"location":"#news","text":"Apr. 26 th , 2023 The ground-truth of our competition dataset is now available here . Apr. 3 rd , 2023 The competition has concluded. Check the results here . Mar. 27 th , 2023 The competition deadline is extended until April 3 rd , 2023 23:59 UTC+1:00 Mar. 20 th , 2023 The competition deadline is extended until March 26 th , 2023 12:00 AM (midnight) Jan. 13 th , 2023 The competition-dataset is now released and submissions are opened on EvalAI. Register here . Dec. 19 th , 2022 This competition is now live and will run until March 20 th , 2023. Find the detailed schedule here .","title":"News"},{"location":"#task-and-resources","text":"We invite you to develop a model that can accurately segment the layout components in document pages as bounding boxes on our competition data-set . The layout prediction accuracy you achieve with your solution will be evaluated on this dataset with our human-annotated layout ground-truth. Code submissions are not required. Find the details here . We highly recommend you to use our recently published DocLayNet dataset for training and internal validation. DocLayNet is highly diverse in layout coverage, and includes Financial reports, Patents, Manuals, Laws, Tenders and Technical Papers. It is human-annotated with 11 distinct layout class labels. Added value for model development is provided through the original PDF pages and a paired JSON representation of the text cells.","title":"Task and resources"},{"location":"#participation","text":"Everyone is welcome to participate in this competition. To ensure fairness, we require teams to abide by the participation rules .","title":"Participation"},{"location":"rules/","text":"Rules \u00b6 To ensure this competition runs under fair conditions, we define the following participation rules. Each participating team must be registered with one and only one account (Registration opens Jan. 16 th , 2023) The maximum team size is 8 people. One team can update their prediction results in COCO format up to 10 times. It is allowed to use additional publicly available third-party data, publicly available pre-trained models, or model ensembling for performance improvement. Participants are not allowed to reverse engineer the ground truth of the competition validation set by any means, which will be treated as cheating and disqualify them from this competition. Winning teams will be decided based on the rank in the leaderboard, which is determined by the mAP score. Winning teams are strongly encouraged to release their code as open source. Team mergers are not allowed.","title":"Rules"},{"location":"rules/#rules","text":"To ensure this competition runs under fair conditions, we define the following participation rules. Each participating team must be registered with one and only one account (Registration opens Jan. 16 th , 2023) The maximum team size is 8 people. One team can update their prediction results in COCO format up to 10 times. It is allowed to use additional publicly available third-party data, publicly available pre-trained models, or model ensembling for performance improvement. Participants are not allowed to reverse engineer the ground truth of the competition validation set by any means, which will be treated as cheating and disqualify them from this competition. Winning teams will be decided based on the rank in the leaderboard, which is determined by the mAP score. Winning teams are strongly encouraged to release their code as open source. Team mergers are not allowed.","title":"Rules"},{"location":"schedule/","text":"Schedule \u00b6 This competition starts on Dec. 19 th , 2022. You are welcome to familiarize with the task and the data assets now. The competition data-set is published and submissions on our EvalAI challenge are opened on Jan. 13 th , 2023. The competition closes on March 20 th , 2023 and the winning team will be announced. Date Milestone 19/Dec/2022 Go-live of competition website with task description, eval metrics and participation rules 13/Jan/2022 Competition data-set is released (without ground truth), team registration opens on EvalAI and submissions can be made 20/Mar/2023 3/Apr/2023 Deadline Extended! Competition ends, winner is determined 21-26/Aug/2023 Results are presented at ICDAR 2023 conference","title":"Schedule"},{"location":"schedule/#schedule","text":"This competition starts on Dec. 19 th , 2022. You are welcome to familiarize with the task and the data assets now. The competition data-set is published and submissions on our EvalAI challenge are opened on Jan. 13 th , 2023. The competition closes on March 20 th , 2023 and the winning team will be announced. Date Milestone 19/Dec/2022 Go-live of competition website with task description, eval metrics and participation rules 13/Jan/2022 Competition data-set is released (without ground truth), team registration opens on EvalAI and submissions can be made 20/Mar/2023 3/Apr/2023 Deadline Extended! Competition ends, winner is determined 21-26/Aug/2023 Results are presented at ICDAR 2023 conference","title":"Schedule"},{"location":"task/","text":"Task \u00b6 You are invited to advance the research in accurately segmenting the layout on a broad range of document styles and domains. To achieve this, we challenge you to develop a model that can correctly identify and segment the layout components in document pages as bounding boxes on a competition data-set we provide. Training resources \u00b6 In our recently published DocLayNet dataset, which contains 80k+ human-annotated document pages exposing diverse layouts, we define 11 classes for layout components (paragraphs, headings, tables, figures, lists, mathematical formulas and several more). We encourage you to use this dataset for training and internal evaluation of your solution. Further, you may consider any other publicly available document layout dataset for training (e.g. PubLayNet , DocBank ). Tip DocLayNet provides not only bitmap page samples and COCO bounding-box annotations, but also JSON files with the digital text-cells from the originating PDFs. This may support you in solving the proposed challenge using both language and vision with multimodal ML approaches. Evaluation \u00b6 Competition data-set \u00b6 To assess the layout segmentation performance of your model, we provide a competition data-set of 498 new pages in the same manner as DocLayNet . This competition data-set includes a balanced mix of corporate document samples as shown below. Samples in the new others category expose additional layouts that fall outside of the DocLayNet layout distribution. Document category Sample count reports 293 manuals 114 patents 48 others 43 total 498 For the purpose of testing your submissions on the EvalAI platform without restrictions, we also provide a development dataset with only 20 samples. Dataset format \u00b6 The competition data-set provides PNG images (1025 x 1025 px) of the pages and paired JSON files with the text cell content and bounding-boxes. For details, please refer to the DocLayNet documentation . \u251c\u2500\u2500 coco.json \u251c\u2500\u2500 PNG \u2502 \u251c\u2500\u2500 <hash>.png \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 JSON \u2502 \u251c\u2500\u2500 <hash>.json \u2502 \u251c\u2500\u2500 ... Note The provided coco.json only contains the definition of images and the class label map (called categories in COCO terms). The annotation ground-truth is ommitted. Please keep in mind that it is not allowed to create ground-truth for the competition data-set and train your model on it. Downloads \u00b6 Asset Download link Competition dataset (498 pages) Download (205 MB) Competition dataset ground-truth(498 pages) Download (205 MB) Development dataset (20 pages) Download (8 MB) Evaluation Metric \u00b6 Your submissions on our EvalAI challenge will be evaluated using the Mean Average Precision (mAP) @ Intersection-over-Union (IoU) [0.50:0.95] metric, as used in the COCO object detection competition. In detail, we will calculate the average precision for a sequence of IoU thresholds ranging from 0.50 to 0.95 with a step size of 0.05. This metric is computed for every document category in the competition-dataset. Then the mean of the average precisions on all categories is computed as the final score. Submission \u00b6 We ask you to upload a JSON file in COCO results format here , with complete layout bounding-boxes for each page sample. The given image_id s must correspond to the ones we publish with the competition data-set's coco.json . For each submission you make, the computed mAP will be provided for each category as well as combined. The leaderboard will be ranked based on the overall mAP. Example submission snippet \u00b6 [ { \"bbox\" : [ // bounding-box in COCO format (x,y,w,h) 105 , 290 , 500 , 120 ], \"category_id\" : 4 , // ID of label class for this bounding-box. Find the `categories` dictionary in the coco.json. Must resolve to one of [Caption, Footnote, Formula, List-item, Page-footer, Page-header, Picture, Section-header, Table, Text, Title]. \"image_id\" : 0 , // ID of dataset sample that this annotation is referring to, find the `images` dictionary in the coco.json \"score\" : 0.77 }, { \"bbox\" : [ 602 , 302 , 354 , 30 ], \"category_id\" : 4 , \"image_id\" : 0 , \"score\" : 0.94 }, ...","title":"Task and Resources"},{"location":"task/#task","text":"You are invited to advance the research in accurately segmenting the layout on a broad range of document styles and domains. To achieve this, we challenge you to develop a model that can correctly identify and segment the layout components in document pages as bounding boxes on a competition data-set we provide.","title":"Task"},{"location":"task/#training-resources","text":"In our recently published DocLayNet dataset, which contains 80k+ human-annotated document pages exposing diverse layouts, we define 11 classes for layout components (paragraphs, headings, tables, figures, lists, mathematical formulas and several more). We encourage you to use this dataset for training and internal evaluation of your solution. Further, you may consider any other publicly available document layout dataset for training (e.g. PubLayNet , DocBank ). Tip DocLayNet provides not only bitmap page samples and COCO bounding-box annotations, but also JSON files with the digital text-cells from the originating PDFs. This may support you in solving the proposed challenge using both language and vision with multimodal ML approaches.","title":"Training resources"},{"location":"task/#evaluation","text":"","title":"Evaluation"},{"location":"task/#competition-data-set","text":"To assess the layout segmentation performance of your model, we provide a competition data-set of 498 new pages in the same manner as DocLayNet . This competition data-set includes a balanced mix of corporate document samples as shown below. Samples in the new others category expose additional layouts that fall outside of the DocLayNet layout distribution. Document category Sample count reports 293 manuals 114 patents 48 others 43 total 498 For the purpose of testing your submissions on the EvalAI platform without restrictions, we also provide a development dataset with only 20 samples.","title":"Competition data-set"},{"location":"task/#dataset-format","text":"The competition data-set provides PNG images (1025 x 1025 px) of the pages and paired JSON files with the text cell content and bounding-boxes. For details, please refer to the DocLayNet documentation . \u251c\u2500\u2500 coco.json \u251c\u2500\u2500 PNG \u2502 \u251c\u2500\u2500 <hash>.png \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 JSON \u2502 \u251c\u2500\u2500 <hash>.json \u2502 \u251c\u2500\u2500 ... Note The provided coco.json only contains the definition of images and the class label map (called categories in COCO terms). The annotation ground-truth is ommitted. Please keep in mind that it is not allowed to create ground-truth for the competition data-set and train your model on it.","title":"Dataset format"},{"location":"task/#downloads","text":"Asset Download link Competition dataset (498 pages) Download (205 MB) Competition dataset ground-truth(498 pages) Download (205 MB) Development dataset (20 pages) Download (8 MB)","title":"Downloads"},{"location":"task/#evaluation-metric","text":"Your submissions on our EvalAI challenge will be evaluated using the Mean Average Precision (mAP) @ Intersection-over-Union (IoU) [0.50:0.95] metric, as used in the COCO object detection competition. In detail, we will calculate the average precision for a sequence of IoU thresholds ranging from 0.50 to 0.95 with a step size of 0.05. This metric is computed for every document category in the competition-dataset. Then the mean of the average precisions on all categories is computed as the final score.","title":"Evaluation Metric"},{"location":"task/#submission","text":"We ask you to upload a JSON file in COCO results format here , with complete layout bounding-boxes for each page sample. The given image_id s must correspond to the ones we publish with the competition data-set's coco.json . For each submission you make, the computed mAP will be provided for each category as well as combined. The leaderboard will be ranked based on the overall mAP.","title":"Submission"},{"location":"task/#example-submission-snippet","text":"[ { \"bbox\" : [ // bounding-box in COCO format (x,y,w,h) 105 , 290 , 500 , 120 ], \"category_id\" : 4 , // ID of label class for this bounding-box. Find the `categories` dictionary in the coco.json. Must resolve to one of [Caption, Footnote, Formula, List-item, Page-footer, Page-header, Picture, Section-header, Table, Text, Title]. \"image_id\" : 0 , // ID of dataset sample that this annotation is referring to, find the `images` dictionary in the coco.json \"score\" : 0.77 }, { \"bbox\" : [ 602 , 302 , 354 , 30 ], \"category_id\" : 4 , \"image_id\" : 0 , \"score\" : 0.94 }, ...","title":"Example submission snippet"},{"location":"team/","text":"Contact us \u00b6 This competition is hosted by the AI 4 Knowledge group in IBM Research, Zurich. For any inquiries, please email us at deepsearch-core@zurich.ibm.com Organizers \u00b6 Christoph Auer Ahmed Nassar Maksym Lysak Michele Dolfi Nikolaos Livathinos Peter Staar","title":"Organizers"},{"location":"team/#contact-us","text":"This competition is hosted by the AI 4 Knowledge group in IBM Research, Zurich. For any inquiries, please email us at deepsearch-core@zurich.ibm.com","title":"Contact us"},{"location":"team/#organizers","text":"Christoph Auer Ahmed Nassar Maksym Lysak Michele Dolfi Nikolaos Livathinos Peter Staar","title":"Organizers"}]}